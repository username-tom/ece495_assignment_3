{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First import pytorch requirements\n",
    "\n",
    "Usually you important everything at the top of your python file or jupyter notebook but I have explanations as we go through this and only import the minimum requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: future in /root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: torchvision in /root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages (0.6.1)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages (from torchvision) (1.12.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages (from torchvision) (4.3.0)\n",
      "Requirement already satisfied: torch==1.5.1 in /root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages (from torchvision) (1.5.1)\n",
      "Requirement already satisfied: olefile in /root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
      "Requirement already satisfied: future in /root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages (from torch==1.5.1->torchvision) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "# pytorch library\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST and verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_resolve_type_from_object(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: object, arg1: torch._C._jit_tree_views.SourceRange, arg2: Callable[[str], function]) -> torch._C.Type\n\nInvoked with: typing.Union[int, NoneType], None, <function try_ann_to_type.<locals>.fake_rcb at 0x7fb14a1e88c8>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-403413f82543>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# torchvision is a computer vision library that pytorch created for common datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# https://pytorch.org/docs/stable/torchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# import MNIST loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/torchvision/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mshufflenetv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msegmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/torchvision/models/detection/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfaster_rcnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmask_rcnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mkeypoint_rcnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/torchvision/models/detection/faster_rcnn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgeneralized_rcnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGeneralizedRCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnchorGenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRPNHead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRegionProposalNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRoIHeads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGeneralizedRCNNTransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackbone_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresnet_fpn_backbone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m def _onnx_heatmaps_to_keypoints_loop(maps, rois, widths_ceil, heights_ceil,\n\u001b[1;32m    212\u001b[0m                                      widths, heights, offset_x, offset_y, num_keypoints):\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb)\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_rcb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0m_rcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jit_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateResolutionCallbackFromClosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_script_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqualified_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_default_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m         \u001b[0;31m# Forward docstrings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mtry_compile_fn\u001b[0;34m(fn, loc)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0;31m# object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0mrcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jit_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateResolutionCallbackFromClosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rcb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrap_cpp_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb)\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_rcb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0m_rcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jit_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateResolutionCallbackFromClosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_script_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqualified_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_default_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m         \u001b[0;31m# Forward docstrings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36m_get_overloads\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2028\u001b[0m     \u001b[0mcompiled_fns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2029\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0moverload_fn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muncompiled_overloads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2030\u001b[0;31m         \u001b[0mcompiled_fns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_compile_function_with_overload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverload_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqual_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2032\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexisting_compiled_fns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36m_compile_function_with_overload\u001b[0;34m(overload_fn, qual_name, impl_fn)\u001b[0m\n\u001b[1;32m   2008\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_compile_function_with_overload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverload_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqual_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpl_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0moverload_decl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_jit_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverload_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m     \u001b[0moverload_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverload_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverload_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m     \u001b[0mimpl_ast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_jit_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m     \u001b[0moverload_defaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverload_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/torch/jit/annotations.py\u001b[0m in \u001b[0;36mget_signature\u001b[0;34m(fn, rcb, loc, is_method)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# because it didn't have any annotations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype_line\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_type_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/torch/jit/annotations.py\u001b[0m in \u001b[0;36mparse_type_line\u001b[0;34m(type_line, rcb, loc)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to parse the return type of a type annotation: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0marg_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mann_to_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mann\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_ann\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marg_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mann_to_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret_ann\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/torch/jit/annotations.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to parse the return type of a type annotation: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0marg_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mann_to_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mann\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_ann\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marg_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mann_to_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret_ann\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/torch/jit/annotations.py\u001b[0m in \u001b[0;36mann_to_type\u001b[0;34m(ann, loc)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mann_to_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0mthe_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_ann_to_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mthe_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mthe_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/torch/jit/annotations.py\u001b[0m in \u001b[0;36mtry_ann_to_type\u001b[0;34m(ann, loc)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfake_rcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mthe_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve_type_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_rcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthe_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mthe_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _resolve_type_from_object(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: object, arg1: torch._C._jit_tree_views.SourceRange, arg2: Callable[[str], function]) -> torch._C.Type\n\nInvoked with: typing.Union[int, NoneType], None, <function try_ann_to_type.<locals>.fake_rcb at 0x7fb14a1e88c8>"
     ]
    }
   ],
   "source": [
    "# torchvision is a computer vision library that pytorch created for common datasets\n",
    "# https://pytorch.org/docs/stable/torchvision\n",
    "import torchvision\n",
    "# import MNIST loader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# https://pytorch.org/docs/stable/torchvision/datasets.html#mnist\n",
    "# torchvision.datasets.MNIST(root, train=True, transform=None, target_transform=None, download=False)\n",
    "mnist_train_data = MNIST('data', train=True, download=True)\n",
    "\n",
    "print(\"Number of training examples:\", len(mnist_train_data))\n",
    "print(\"Each with one image and one ground truth (gt) label:\", len(mnist_train_data[0]))\n",
    "\n",
    "print(\"One image and one label (number)\")\n",
    "print(mnist_train_data[0][0])\n",
    "print(mnist_train_data[0][1])\n",
    "\n",
    "print(\"Lets take a look at the image size\")\n",
    "print(mnist_train_data[0][0].size)\n",
    "\n",
    "print(\"Lastly let's verify that the image and gt label match\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img, label = mnist_train_data[0]\n",
    "\n",
    "fig = plt.figure(figsize=(1,1))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "plt.title(label)\n",
    "ax1.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training transform and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-06b521c616f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# https://pytorch.org/docs/stable/torchvision/transforms.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Take note of transforms.ToPILImage and transforms.ToTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# We want to give our neural network more exmaples. What transforms make sense for this data?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "# import data transforms\n",
    "# https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "# Take note of transforms.ToPILImage and transforms.ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# We want to give our neural network more exmaples. What transforms make sense for this data?\n",
    "# ColorJitter? Nope, the number is black and white\n",
    "# Crop or Flip? Nope, that could remove parts of the number or make it unreadable\n",
    "# RandomRotation? Yes, numbers can be slightly slanted\n",
    "# Normalize? Yes, you should always normalize\n",
    "\n",
    "mnist_train_tf = transforms.Compose([\n",
    "    transforms.RandomRotation([-10, 10]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # Supplied with dataset (mean, std)\n",
    "])\n",
    "\n",
    "# Let's test out the transform (without toTensor)\n",
    "img_tfed = transforms.RandomRotation([90, 90])(img)\n",
    "# Display the image\n",
    "fig = plt.figure(figsize=(1,1))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "plt.title(label)\n",
    "ax1.imshow(img_tfed, cmap=\"gray\")\n",
    "\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# reimport MNIST with transform\n",
    "mnist_train_data_transformed = MNIST('data', train=True, download=True, transform=mnist_train_tf)\n",
    "\n",
    "# epoch = one iteration over the entire dataset\n",
    "# batch size = samples per batch within an epoch\n",
    "# shuffle = Set to True if your data is ordered\n",
    "# num_workers = number of subprocess to use when dataloading, set higher if you have many cpu cores\n",
    "train_loader = DataLoader(mnist_train_data_transformed, batch_size=4, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important concepts to remember before creating the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel - Convolution: https://en.wikipedia.org/wiki/Kernel_(image_processing)#Convolution\n",
    "- Weighted sum around a pixel\n",
    "- nn.Conv2d(in_channels, out_channels, kernel_size, stride=1)\n",
    "- nn.Conv2d(1, 32, 3, 1)\n",
    "\n",
    "Activation functions:https://en.wikipedia.org/wiki/Activation_function\n",
    "- Logistic - https://en.wikipedia.org/wiki/Logistic_function\n",
    "- ReLU - https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "\n",
    "Pooling: https://pytorch.org/docs/stable/nn.functional.html#pooling-functions\n",
    "- Downsample the feature map\n",
    "- Notable pooling functions: average and max\n",
    "\n",
    "Dropout: https://pytorch.org/docs/stable/nn.html#dropout-layers\n",
    "- Promote independence between feature maps\n",
    "- The dropout paper https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "\n",
    "Fully connected layers\n",
    "- First flatten the tensor\n",
    "- Goal is to classify the image based on given feature maps\n",
    "- Eventual output is the number of classes\n",
    "\n",
    "Final output layer\n",
    "- We want each class index to have probability\n",
    "- Can we use softmax? Takes vector and makes sum 1. https://pytorch.org/docs/stable/nn.html#torch.nn.Softmax no it has some issues with NLL loss https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/\n",
    "- log softmax https://pytorch.org/docs/stable/nn.html#torch.nn.LogSoftmax\n",
    "\n",
    "\n",
    "Visualizing and Understanding Convolutional Networks\n",
    "https://arxiv.org/abs/1311.2901\n",
    "- Lines -> Lines/Curves/Corners -> Patterns -> Objects\n",
    "\n",
    "Even more detail:\n",
    "\n",
    "http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Convolutional_neural_network#Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network layers (Conv2d, Linear, etc.) that will be trained\n",
    "# https://pytorch.org/docs/stable/nn.html\n",
    "import torch.nn as nn\n",
    "# Many functions (convolution, pooling, activation, etc.)\n",
    "# https://pytorch.org/docs/stable/nn.functional.html\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# This NN implementation is from the pytorch github\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "\n",
    "# all nn layers defined and stored in vars within __init__\n",
    "# https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d\n",
    "# torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # comments were created by: print(x.shape)\n",
    "        # initial x shape: torch.Size([4, 1, 28, 28])\n",
    "        # 4 = batch_size\n",
    "        # 1 = number of channels (Greyscale will have 1, RGB with have 3)\n",
    "        # 28 = height\n",
    "        # 28 = width\n",
    "        x = self.conv1(x)       # torch.Size([4, 32, 26, 26])\n",
    "        x = F.relu(x)           # torch.Size([4, 32, 26, 26])\n",
    "        x = self.conv2(x)       # torch.Size([4, 64, 24, 24])\n",
    "        x = F.max_pool2d(x, 2)  # torch.Size([4, 64, 12, 12])\n",
    "        x = self.dropout1(x)    # torch.Size([4, 64, 12, 12])\n",
    "        x = torch.flatten(x, 1) # torch.Size([4, 9216])\n",
    "        x = self.fc1(x)         # torch.Size([4, 128])\n",
    "        x = F.relu(x)           # torch.Size([4, 128])\n",
    "        x = self.dropout2(x)    # torch.Size([4, 128])\n",
    "        x = self.fc2(x)         # torch.Size([4, 10])\n",
    "        output = F.log_softmax(x, dim=1) # torch.Size([4, 10])\n",
    "        return output\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# Overide device when testing on CPU\n",
    "# device = 'cpu'\n",
    "\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various optimization algorithms are in:\n",
    "# https://pytorch.org/docs/stable/optim.html\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# NLLLoss is used when you expect 1 of N classes\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "# https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n",
    "# torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "# lr is the learning rate, how much to change weights based on current values\n",
    "# momentum will change how much to change weights based on previous changes\n",
    "# https://www.quora.com/What-is-the-difference-between-momentum-and-learning-rate\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(4):  # loop over the dataset twice\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, batch_idx + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Test loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data loading should not modify the image \n",
    "mnist_test_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # Supplied with dataset (mean, std)\n",
    "])\n",
    "mnist_test_data = MNIST('data', train=False, download=True, transform=mnist_test_tf)\n",
    "\n",
    "test_loader = DataLoader(mnist_test_data, batch_size=4, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = net(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find one correctly classified and one misclassified\n",
    "\n",
    "found_bad = False\n",
    "found_good = False\n",
    "bad_example = None\n",
    "good_example = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = net(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        \n",
    "        for i in range (0,len(target)):\n",
    "            if not found_good and target[i] == pred[i]:\n",
    "                good_example = transforms.ToPILImage()(data[i].cpu()), target[i].cpu().numpy(), pred[i][0].cpu().numpy()\n",
    "                found_good = True\n",
    "            if not found_bad and target[i] != pred[i]:\n",
    "                bad_example = transforms.ToPILImage()(data[i].cpu()), target[i].cpu().numpy(), pred[i][0].cpu().numpy()\n",
    "                found_bad = True\n",
    "        \n",
    "        if found_bad and found_good:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target_label, pred_label = good_example\n",
    "\n",
    "fig = plt.figure(figsize=(1,1))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "plt.title(\"Predicted label: \" + str(pred_label) + \" Target label: \" + str(target_label))\n",
    "ax1.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target_label, pred_label = bad_example\n",
    "\n",
    "fig = plt.figure(figsize=(1,1))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "plt.title(\"Predicted label: \" + str(pred_label) + \" Target label: \" + str(target_label))\n",
    "ax1.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
